#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AION_8_1 usecase.
File generation time: 2022-07-14 13:59:24
'''
#Standard Library modules
import json
import argparse
import platform
import logging
import shutil

#Third Party modules
from pathlib import Path
import pandas as pd 

IOFiles = {
    "rawData": "rawData.dat",
    "prodGrndTruData": "prodDataGT.dat",
    "prodData": "prodData.dat",
    "metaData": "modelMetaData.json",
    "log": "aion.log",
    "outputData": "rawData.dat",
    "monitoring": "monitoring.json"
}
                    
def read_json(file_path):                    
    data = None                    
    with open(file_path,'r') as f:                    
        data = json.load(f)                    
    return data                    
                    
def write_json(data, file_path):                    
    with open(file_path,'w') as f:                    
        json.dump(data, f)                    
                    
def read_data(file_path, encoding='utf-8', sep=','):                    
    return pd.read_csv(file_path, encoding=encoding, sep=sep)                    
                    
def write_data(data, file_path, index=False):                    
    return data.to_csv(file_path, index=index)                    
                    
#Uncomment and change below code for google storage                    
#def write_data(data, file_path, index=False):                    
#    file_name= file_path.name                    
#    data.to_csv('output_data.csv')                    
#    storage_client = storage.Client()                    
#    bucket = storage_client.bucket('aion_data')                    
#    bucket.blob('prediction/'+file_name).upload_from_filename('output_data.csv', content_type='text/csv')                    
#    return data                    
                    
def is_file_name_url(file_name):                    
    supported_urls_starts_with = ('gs://','https://','http://')                    
    return file_name.startswith(supported_urls_starts_with)                    

                    
log = None                    
def set_logger(log_file, mode='a'):                    
    global log                    
    logging.basicConfig(filename=log_file, filemode=mode, format='%(asctime)s %(name)s- %(message)s', level=logging.INFO, datefmt='%d-%b-%y %H:%M:%S')                    
    log = logging.getLogger(Path(__file__).parent.name)                    
    return log                    
                    
def get_logger():                    
    return log

                    
def log_dataframe(df, msg=None):                    
    import io                    
    buffer = io.StringIO()                    
    df.info(buf=buffer)                    
    if msg:                    
        log_text = f'Data frame after {msg}:'                    
    else:                    
        log_text = 'Data frame:'                    
    log_text += '\n\t'+str(df.head(2)).replace('\n','\n\t')                    
    log_text += ('\n\t' + buffer.getvalue().replace('\n','\n\t'))                    
    get_logger().info(log_text)
                    
def add_file_for_production(meta_data, file):                    
    if 'prod_files' not in meta_data.keys():                    
        meta_data['prod_files'] = []                    
    if file not in meta_data['prod_files']:                    
        meta_data['prod_files'].append(file)                    
                    
def copy_prod_files(source, target, meta_data):                    
    if 'prod_files' in meta_data.keys():                    
        for file in meta_data['prod_files']:                    
            if not (target/file).exists():                    
                if (source/file).exists():                    
                    shutil.copy(source/file, target/file)
        
def validateConfig():        
    config_file = Path(__file__).parent/'config.json'        
    if not Path(config_file).exists():        
        raise ValueError(f'Config file is missing: {config_file}')        
    config = read_json(config_file)		
    if not config['targetPath']:        
        raise ValueError(f'Target Path is not configured')        
    return config

#This function will read the data and save the data on persistent storage        
def load_data():        
    config = validateConfig()
    if platform.system() == 'Windows':        
        targetPath = Path(config['targetPath'])
    else:        
        targetPath = Path('/aion')/config['targetPath']       
    targetPath.mkdir(parents=True, exist_ok=True)	
    log_file = targetPath/IOFiles['log']        
    logger = set_logger(log_file)
    monitoring = targetPath/IOFiles['monitoring']	
    if monitoring.exists():        
        monitoringStatus = read_json(monitoring)
        if monitoringStatus['dataLocation'] == '' and monitoringStatus['driftStatus'] != 'No Drift':
            actual_data_location = targetPath/IOFiles['prodGrndTruData']
            predict_data_location = targetPath/IOFiles['prodData']
            raw_data_location = targetPath/IOFiles['rawData']		
            if actual_data_location.exists() and predict_data_location.exists(): 		
                predicted_data = pd.read_csv(predict_data_location)        		
                actual_data_path = pd.read_csv(actual_data_location)
                common_col = [k for k in predicted_data.columns.tolist() if k in actual_data_path.columns.tolist()]				
                mergedRes = pd.merge(actual_data_path, predicted_data, on =common_col,how = 'inner')
                raw_data_path = pd.read_csv(raw_data_location)			
                df = pd.concat([raw_data_path,mergedRes])
            else:
                raise ValueError(f'Prod Data not found')			
        elif monitoringStatus['dataLocation'] == '':
            raise ValueError(f'Data Location does not exist')
        else:
            location = monitoringStatus['dataLocation']
            get_logger().info(f'Dataset path: {location}')        
            df = read_data(location)			
    else:
        raise ValueError(f'Monitoring.json does not exist')		

    status = {}        
    output_data_path = targetPath/IOFiles['outputData']       
    log_dataframe(df)        
    required_features = config['selected_features'] + [config['target_feature']]        
    get_logger().info('Dataset features required: ' + ','.join(required_features))        
    missing_features = [x for x in required_features if x not in df.columns.tolist()]        
    if missing_features:        
        raise ValueError(f'Some feature/s is/are missing: {missing_features}')        
    get_logger().info('Removing unused features: '+','.join(list(set(df.columns) - set(required_features))))        
    df = df[required_features]        
    get_logger().info(f'Required features: {required_features}')        
    try:        
        get_logger().info(f'Saving Dataset: {str(output_data_path)}')        
        write_data(df, output_data_path, index=False)        
        status = {'Status':'Success','DataFilePath':IOFiles['outputData']}        
    except:        
        raise ValueError('Unable to create data file')        
        
    meta_data_file = targetPath/IOFiles['metaData']        
    meta_data = dict()        
    meta_data['load_data'] = {}        
    meta_data['load_data']['selected_features'] = [x for x in config['selected_features'] if x != config['target_feature']]        
    meta_data['load_data']['Status'] = status        
    write_json(meta_data, meta_data_file)        
    output = json.dumps(status)        
    get_logger().info(output)        
    return output

        
if __name__ == '__main__':        
    try:        
        print(load_data())        
    except Exception as e:        
        if get_logger():        
            get_logger().error(e, exc_info=True)        
        status = {'Status':'Failure','Message':str(e)}        
        print(json.dumps(status))        