#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AION_8_1 usecase.
File generation time: 2022-07-14 13:59:25
'''
#Standard Library modules
import platform
import time
import logging
import shutil
import json
import sys
import argparse

#Third Party modules
from pathlib import Path
import pandas as pd 
import word2number as w2n 
import numpy as np 
from sklearn.preprocessing import LabelEncoder
import joblib
from sklearn.impute import SimpleImputer
from category_encoders import TargetEncoder

IOFiles = {
    "inputData": "rawData.dat",
    "metaData": "modelMetaData.json",
    "log": "aion.log",
    "outputData": "transformedData.dat",
    "targetEncoder": "targetEncoder.pkl",
    "featureEncoder": "inputEncoder.pkl"
}
                    
def read_json(file_path):                    
    data = None                    
    with open(file_path,'r') as f:                    
        data = json.load(f)                    
    return data                    
                    
def write_json(data, file_path):                    
    with open(file_path,'w') as f:                    
        json.dump(data, f)                    
                    
def read_data(file_path, encoding='utf-8', sep=','):                    
    return pd.read_csv(file_path, encoding=encoding, sep=sep)                    
                    
def write_data(data, file_path, index=False):                    
    return data.to_csv(file_path, index=index)                    
                    
#Uncomment and change below code for google storage                    
#def write_data(data, file_path, index=False):                    
#    file_name= file_path.name                    
#    data.to_csv('output_data.csv')                    
#    storage_client = storage.Client()                    
#    bucket = storage_client.bucket('aion_data')                    
#    bucket.blob('prediction/'+file_name).upload_from_filename('output_data.csv', content_type='text/csv')                    
#    return data                    
                    
def is_file_name_url(file_name):                    
    supported_urls_starts_with = ('gs://','https://','http://')                    
    return file_name.startswith(supported_urls_starts_with)                    

                    
log = None                    
def set_logger(log_file, mode='a'):                    
    global log                    
    logging.basicConfig(filename=log_file, filemode=mode, format='%(asctime)s %(name)s- %(message)s', level=logging.INFO, datefmt='%d-%b-%y %H:%M:%S')                    
    log = logging.getLogger(Path(__file__).parent.name)                    
    return log                    
                    
def get_logger():                    
    return log

                    
def add_file_for_production(meta_data, file):                    
    if 'prod_files' not in meta_data.keys():                    
        meta_data['prod_files'] = []                    
    if file not in meta_data['prod_files']:                    
        meta_data['prod_files'].append(file)                    
                    
def copy_prod_files(source, target, meta_data):                    
    if 'prod_files' in meta_data.keys():                    
        for file in meta_data['prod_files']:                    
            if not (target/file).exists():                    
                if (source/file).exists():                    
                    shutil.copy(source/file, target/file)
                    
def s2n(value):                    
  try:                    
      x=eval(value)                    
      return x                    
  except:                    
      try:                    
          return w2n.word_to_num(value)                    
      except:                    
          return np.nan
                    
def log_dataframe(df, msg=None):                    
    import io                    
    buffer = io.StringIO()                    
    df.info(buf=buffer)                    
    if msg:                    
        log_text = f'Data frame after {msg}:'                    
    else:                    
        log_text = 'Data frame:'                    
    log_text += '\n\t'+str(df.head(2)).replace('\n','\n\t')                    
    log_text += ('\n\t' + buffer.getvalue().replace('\n','\n\t'))                    
    get_logger().info(log_text)
        
def validateConfig():        
    config_file = Path(__file__).parent/'config.json'        
    if not Path(config_file).exists():        
        raise ValueError(f'Config file is missing: {config_file}')        
    config = read_json(config_file)        
    return config


def transformation():        
    config = validateConfig()        
    if platform.system() == 'Windows':		
        targetPath = Path(config['targetPath'])		
    else:		
        targetPath = Path('/aion')/config['targetPath']        
    if not targetPath.exists():        
        raise ValueError(f'targetPath does not exist')        
    meta_data_file = targetPath/IOFiles['metaData']        
    if meta_data_file.exists():        
        meta_data = read_json(meta_data_file)        
    else:        
        raise ValueError(f'Configuration file not found: {meta_data_file}')        
    log_file = targetPath/IOFiles['log']        
    logger = set_logger(log_file)        
    dataLoc = targetPath/IOFiles['inputData']        
    if not dataLoc.exists():        
        return {'Status':'Failure','Message':'Data location does not exists.'}        
    add_file_for_production(meta_data, IOFiles['inputData'])        
    status = dict()        
    df = read_data(dataLoc)        
    log_dataframe(df)        
        
    target_feature = config['target_feature']        
    train_features = [x for x in config['train_features'] if x != target_feature]        
    num_features = [x for x in config['num_features'] if x != target_feature]        
        
    df = df[[target_feature] + train_features]        
    meta_data['transformation'] = {}        
    df = df.dropna(axis=0, subset=[target_feature])        
    df = df.dropna(axis=0, how='all', subset=df.columns)        
    df = df.drop_duplicates(keep='first')        
    df = df.reset_index(drop=True)
    for feat in config['word2num_features']:
        df[ feat ] = df[feat].apply(lambda x: s2n(x))
    log_dataframe(df)
    meta_data['transformation']['word2num_features'] = config['word2num_features']
    target_encoder = LabelEncoder()
    df[config['target_feature']] = target_encoder.fit_transform(df[config['target_feature']])
    target_encoder_file_name = str(targetPath/IOFiles['targetEncoder'])
    add_file_for_production(meta_data, IOFiles['targetEncoder'])
    joblib.dump(target_encoder, target_encoder_file_name)
    meta_data['transformation']['target_encoder'] = IOFiles['targetEncoder']
    logger.info('Categorical to numeric conversion done for target feature')
    meta_data['transformation']['fillna'] = {}
    cat_features = [x for x in config['cat_features'] if x != target_feature]
    cat_imputer = SimpleImputer(strategy='most_frequent')
    df[cat_features + config['cat_num_features']] = cat_imputer.fit_transform(df[cat_features + config['cat_num_features']])
    for index, col in enumerate(cat_features + config['cat_num_features']):
        meta_data['transformation']['fillna'][col] = cat_imputer.statistics_[index]
    logger.info(f'Missing values replaced for categorical columns {cat_features}')
    meta_data['transformation']['cat_encoder'] = {}
    cat_encoder = TargetEncoder(cols=cat_features)
    df[train_features] = cat_encoder.fit_transform(df[train_features],df[target_feature])
    cat_encoder_file = str(targetPath/IOFiles['featureEncoder'])
    add_file_for_production(meta_data, IOFiles['featureEncoder'])
    joblib.dump(cat_encoder, cat_encoder_file)
    meta_data['transformation']['cat_encoder']['file'] = IOFiles['featureEncoder']
    meta_data['transformation']['cat_encoder']['features'] = cat_features
    logger.info('Categorical to numeric conversion done')
    num_imputer = SimpleImputer(strategy='median')
    df[num_features] = num_imputer.fit_transform(df[num_features])
    for index, col in enumerate(num_features):
        meta_data['transformation']['fillna'][col] = num_imputer.statistics_[index]
    logger.info(f'Missing values replaced for numeric columns {num_features}')                
    log_dataframe(df)                
    csv_path = str(targetPath/IOFiles['outputData'])                
    write_data(df, csv_path,index=False)                
    status = {'Status':'Success','DataFilePath':IOFiles['outputData'], 'text_profiler':{}}                
    meta_data['transformation']['Status'] = status                
    write_json(meta_data, str(targetPath/IOFiles['metaData']))                
    logger.info(f'Transformed data saved at {csv_path}')                
    logger.info(f'output: {status}')                
    return json.dumps(status)
        
if __name__ == '__main__':        
    try:        
        print(transformation())        
    except Exception as e:        
        if get_logger():        
            get_logger().error(e, exc_info=True)        
        status = {'Status':'Failure','Message':str(e)}        
        print(json.dumps(status))